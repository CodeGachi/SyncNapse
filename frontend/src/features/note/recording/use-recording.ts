/**
 * ì‹¤ì œ ë…¹ìŒ ê¸°ëŠ¥ í›…
 * MediaRecorder API + Web Speech APIë¥¼ ì‚¬ìš©í•œ ë…¹ìŒ ë° ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹
 */

"use client";

import { useState, useRef, useCallback, useEffect } from "react";
import { useQueryClient } from "@tanstack/react-query";
import { SpeechRecognitionService, type SpeechSegment } from "@/lib/speech/speech-recognition";
import { useScriptTranslationStore } from "@/stores";
import type { WordWithTime } from "@/lib/types";
import * as transcriptionApi from "@/lib/api/transcription.api";
import * as audioApi from "@/lib/api/audio.api";
import { createLogger } from "@/lib/utils/logger";

const log = createLogger("Recording");

export interface RecordingData {
  id: string;
  title: string;
  audioBlob: Blob;
  duration: number;
  createdAt: Date;
  sessionId?: string; // Backend transcription session ID
  audioRecordingId?: string; // Backend audio recording ID (for timeline events)
}

/**
 * í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì •
 * @param text - ì„¸ê·¸ë¨¼íŠ¸ì˜ ì „ì²´ í…ìŠ¤íŠ¸
 * @param startTime - ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ì‹œê°„ (ì´ˆ)
 * @param confidence - ì¸ì‹ ì‹ ë¢°ë„ (0-1)
 * @returns ì¶”ì •ëœ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ë‹¨ì–´ ë°°ì—´
 */
function estimateWordTimestamps(
  text: string,
  startTime: number,
  confidence: number = 1.0
): WordWithTime[] {
  const words = text.trim().split(/\s+/).filter(word => word.length > 0);
  if (words.length === 0) return [];

  // Estimate average speaking rate: ~2.5 words per second (Korean/English average)
  const avgWordsPerSecond = 2.5;
  const estimatedDuration = words.length / avgWordsPerSecond;
  const timePerWord = estimatedDuration / words.length;

  const wordTimestamps: WordWithTime[] = [];
  for (let i = 0; i < words.length; i++) {
    wordTimestamps.push({
      word: words[i],
      startTime: startTime + (i * timePerWord),
      confidence: confidence,
      wordIndex: i,
    });
  }

  log.debug('ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì •:', {
    text: text.substring(0, 50),
    wordCount: words.length,
    startTime,
    estimatedDuration: estimatedDuration.toFixed(2) + 's'
  });

  return wordTimestamps;
}

export function useRecording(noteId?: string | null) {
  const [isRecording, setIsRecording] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [recordingTime, setRecordingTime] = useState(0);
  const [recordingStartTime, setRecordingStartTime] = useState<Date | null>(null); // Track when recording started
  const [error, setError] = useState<string | null>(null);
  const [isSaving, setIsSaving] = useState(false);
  const [audioRecordingId, setAudioRecordingId] = useState<string | null>(null); // AudioRecording ID for timeline

  const queryClient = useQueryClient();
  const { setScriptSegments, clearScriptSegments} = useScriptTranslationStore();

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const speechRecognitionRef = useRef<SpeechRecognitionService | null>(null);

  // Track recording state for cleanup (prevent stale closure)
  const isRecordingRef = useRef(false);

  // Track pause time for timestamp correction
  const pauseStartTimeRef = useRef<number>(0);
  const totalPausedTimeRef = useRef<number>(0);

  // Track when Speech Recognition was (re)started for accurate timestamp calculation
  const speechRecognitionStartTimeRef = useRef<number>(0);
  const audioRecordingTimeAtSpeechStartRef = useRef<number>(0);
  
  // Store all segments (including interim) for real-time display
  const segmentsMapRef = useRef<Map<string, any>>(new Map());

  // ë…¹ìŒ ì‹œì‘
  const startRecording = useCallback(async () => {
    try {
      setError(null);
      const startTime = new Date();
      setRecordingStartTime(startTime); // Record start time
      setAudioRecordingId(null); // Reset audio recording ID

      // Clear previous script segments and reset timing
      clearScriptSegments();
      segmentsMapRef.current.clear();
      totalPausedTimeRef.current = 0;
      pauseStartTimeRef.current = 0;
      speechRecognitionStartTimeRef.current = 0;
      audioRecordingTimeAtSpeechStartRef.current = 0;
      log.debug('ì´ì „ ìŠ¤í¬ë¦½íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ì •ë¦¬ ë° íƒ€ì´ë° ë¦¬ì…‹');

      // Create AudioRecording for timeline events (if noteId exists)
      let createdAudioRecordingId: string | null = null;
      if (noteId) {
        try {
          const defaultTitle = `${startTime.getFullYear()}_${String(startTime.getMonth() + 1).padStart(2, '0')}_${String(startTime.getDate()).padStart(2, '0')}_${String(startTime.getHours()).padStart(2, '0')}:${String(startTime.getMinutes()).padStart(2, '0')}:${String(startTime.getSeconds()).padStart(2, '0')}`;
          const audioRecording = await audioApi.createRecording({
            noteId,
            title: defaultTitle,
          });
          createdAudioRecordingId = audioRecording.id;
          setAudioRecordingId(audioRecording.id);
          log.debug('íƒ€ì„ë¼ì¸ìš© AudioRecording ìƒì„±ë¨:', audioRecording.id);
        } catch (audioRecordingError) {
          log.error('AudioRecording ìƒì„± ì‹¤íŒ¨:', audioRecordingError);
          // Continue recording even if AudioRecording creation fails
        }
      }

      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ (ìµœì í™”ëœ ì„¤ì •)
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 48000, // 48kHz ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê³ í’ˆì§ˆ)
          channelCount: 1, // ëª¨ë…¸ ì˜¤ë””ì˜¤ (ìŠ¤í…Œë ˆì˜¤ ë¶ˆí•„ìš”)
        }
      });
      
      log.debug('ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ íšë“:', {
        sampleRate: stream.getAudioTracks()[0]?.getSettings().sampleRate,
        channelCount: stream.getAudioTracks()[0]?.getSettings().channelCount,
      });

      streamRef.current = stream;

      // MediaRecorder ì„¤ì •
      const mimeType = MediaRecorder.isTypeSupported("audio/webm")
        ? "audio/webm"
        : "audio/mp4";

      // ì•ˆì •ì ì¸ ë…¹ìŒì„ ìœ„í•œ ì˜µì…˜ ì„¤ì •
      const mediaRecorder = new MediaRecorder(stream, { 
        mimeType,
        audioBitsPerSecond: 128000, // 128kbps - ê³ í’ˆì§ˆ ì˜¤ë””ì˜¤
      });
      
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      // ë°ì´í„° ìˆ˜ì§‘ - ëŠê¹€ ì—†ëŠ” ì˜¤ë””ì˜¤ë¥¼ ìœ„í•œ ì—°ì† ìˆ˜ì§‘
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
          log.debug('ì˜¤ë””ì˜¤ ì²­í¬ ìˆ˜ì§‘:', event.data.size, 'bytes',
                      '| ì´ ì²­í¬:', audioChunksRef.current.length,
                      '| ì´ í¬ê¸°:', audioChunksRef.current.reduce((sum, chunk) => sum + chunk.size, 0), 'bytes');
        }
      };

      // ë…¹ìŒ ìƒíƒœ ë¡œê¹…
      mediaRecorder.onstart = () => {
        log.debug('MediaRecorder ì‹œì‘ë¨');
      };

      mediaRecorder.onpause = () => {
        log.debug('MediaRecorder ì¼ì‹œì •ì§€ë¨');
      };

      mediaRecorder.onresume = () => {
        log.debug('MediaRecorder ì¬ê°œë¨');
      };

      mediaRecorder.onstop = () => {
        log.debug('MediaRecorder ì¤‘ì§€ë¨, ì´ ì²­í¬:', audioChunksRef.current.length);
      };

      mediaRecorder.onerror = (event: any) => {
        log.error('MediaRecorder ì˜¤ë¥˜:', event.error);
      };

      // Initialize Web Speech API for transcription
      try {
        const speechRecognition = new SpeechRecognitionService(
          {
            language: 'ko-KR', // Default to Korean, can be made configurable
            continuous: true,
            interimResults: true,
          },
          {
            onSegment: (segment: SpeechSegment) => {
              // Calculate actual recording time based on Speech Recognition restart
              // segment.startTime is relative to when Speech Recognition (re)started
              // We need to add the audio recording time at that point
              const actualRecordingTime = audioRecordingTimeAtSpeechStartRef.current + segment.startTime;
              
              log.debug('ìŒì„± ì„¸ê·¸ë¨¼íŠ¸:', {
                text: segment.text.substring(0, 30) + '...',
                segmentStartTime: segment.startTime.toFixed(2) + 's',
                audioTimeAtSpeechStart: audioRecordingTimeAtSpeechStartRef.current.toFixed(2) + 's',
                actualRecordingTime: actualRecordingTime.toFixed(2) + 's',
                isPartial: segment.isPartial,
              });

              const words = estimateWordTimestamps(
                segment.text,
                actualRecordingTime,
                segment.confidence || 1.0
              );

              const scriptSegment = {
                id: segment.isPartial ? `interim-${actualRecordingTime}` : segment.id,
                timestamp: actualRecordingTime * 1000, // Convert seconds to milliseconds
                originalText: segment.text,
                words: words,
                speaker: undefined,
                isPartial: segment.isPartial, // Mark for UI to handle
              };

              // Real-time update: Store all segments (interim + final)
              if (segment.isPartial) {
                // Update or add interim segment
                segmentsMapRef.current.set(scriptSegment.id, scriptSegment);
              } else {
                // Remove any interim with similar timestamp and add final
                Array.from(segmentsMapRef.current.keys()).forEach(key => {
                  if (key.startsWith('interim-')) {
                    const interimTime = parseFloat(key.replace('interim-', ''));
                    if (Math.abs(interimTime - actualRecordingTime) < 1.0) {
                      segmentsMapRef.current.delete(key);
                    }
                  }
                });
                segmentsMapRef.current.set(scriptSegment.id, scriptSegment);
                log.debug('ì„¸ê·¸ë¨¼íŠ¸ í™•ì •ë¨:', words.length, 'ê°œ ë‹¨ì–´');
              }

              // Update script store with all segments (sorted by time)
              const allSegments = Array.from(segmentsMapRef.current.values())
                .sort((a, b) => a.timestamp - b.timestamp);
              setScriptSegments(allSegments);
            },
            onError: (error) => {
              log.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', error);
              // ìŒì„± ì¸ì‹ ì˜¤ë¥˜ ì‹œ ë…¹ìŒì€ ê³„ì†
            },
          }
        );

        speechRecognitionRef.current = speechRecognition;
        speechRecognition.start();
        
        // ìŒì„± ì¸ì‹ ì‹œì‘ ì‹œê°„ê³¼ í˜„ì¬ ì˜¤ë””ì˜¤ ë…¹ìŒ ì‹œê°„ ê¸°ë¡
        speechRecognitionStartTimeRef.current = Date.now();
        audioRecordingTimeAtSpeechStartRef.current = 0; // ìƒˆë¡œ ì‹œì‘
        log.debug('ìŒì„± ì¸ì‹ ì‹œì‘ë¨, ë…¹ìŒ ì‹œê°„: 0s');
      } catch (speechError) {
        log.error('ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:', speechError);
        // ìŒì„± ì¸ì‹ ì‹¤íŒ¨í•´ë„ ë…¹ìŒì€ ê³„ì†
      }

      // ë…¹ìŒ ì‹œì‘ (timeslice ì—†ì´ - /transcription í˜ì´ì§€ì™€ ë™ì¼í•œ ë°©ì‹)
      // timeslice ì—†ì´ ì‹œì‘í•˜ì—¬ stop() í˜¸ì¶œ ì‹œ ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ìˆ˜ì§‘
      // ì´ ë°©ì‹ì´ ê°€ì¥ ì•ˆì •ì ì´ê³  ëŠê¹€ì´ ì—†ìŒ (webm íŒŒì¼ ë¬´ê²°ì„± ë³´ì¥)
      mediaRecorder.start();
      log.debug('MediaRecorder ì‹œì‘ë¨ (timeslice ì—†ì´ - /transcription íŒ¨í„´ ì¤€ìˆ˜)');
      setIsRecording(true);
      isRecordingRef.current = true; // ğŸ”¥ FIX: Update ref
      setIsPaused(false);
      setRecordingTime(0);

      // íƒ€ì´ë¨¸ ì‹œì‘
      timerRef.current = setInterval(() => {
        setRecordingTime((prev) => prev + 1);
      }, 1000);

    } catch (err) {
      log.error("ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:", err);
      setError("ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤");
    }
  }, [clearScriptSegments, setScriptSegments]);

  // ë…¹ìŒ ì¼ì‹œì •ì§€
  const pauseRecording = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      // Request current data before pause to prevent data loss
      mediaRecorderRef.current.requestData();
      mediaRecorderRef.current.pause();
      setIsPaused(true);
      
      // ì¼ì‹œì •ì§€ ì‹œì‘ ì‹œê°„ ì¶”ì 
      pauseStartTimeRef.current = Date.now();
      log.debug('ì¼ì‹œì •ì§€ë¨:', pauseStartTimeRef.current,
                  '| ìˆ˜ì§‘ëœ ì²­í¬:', audioChunksRef.current.length);

      // ì¼ì‹œì •ì§€ ì¤‘ ìŒì„± ì¸ì‹ ì¤‘ì§€
      if (speechRecognitionRef.current) {
        speechRecognitionRef.current.stop();
        speechRecognitionRef.current = null; // ì¬ì‹œì‘ í—ˆìš©ì„ ìœ„í•´ ë¦¬ì…‹
        log.debug('ì¼ì‹œì •ì§€ë¥¼ ìœ„í•´ ìŒì„± ì¸ì‹ ì¤‘ì§€ë¨');
      }

      if (timerRef.current) {
        clearInterval(timerRef.current);
        timerRef.current = null;
      }
    }
  }, []);

  // ë…¹ìŒ ì¬ê°œ
  const resumeRecording = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "paused") {
      mediaRecorderRef.current.resume();
      setIsPaused(false);

      // ì¼ì‹œì •ì§€ ì‹œê°„ì„ ì´í•©ì— ì¶”ê°€
      const pauseDuration = Date.now() - pauseStartTimeRef.current;
      totalPausedTimeRef.current += pauseDuration;
      log.debug('ì¬ê°œë¨:', (pauseDuration / 1000).toFixed(2) + 's í›„',
                  '| ì´ ì¼ì‹œì •ì§€ ì‹œê°„:', (totalPausedTimeRef.current / 1000).toFixed(2) + 's');

      // Restart speech recognition with same callbacks
      if (!speechRecognitionRef.current && streamRef.current) {
        try {
          const speechRecognition = new SpeechRecognitionService(
            {
              language: 'ko-KR',
              continuous: true,
              interimResults: true,
            },
            {
              onSegment: (segment: SpeechSegment) => {
                // Calculate actual recording time based on Speech Recognition restart
                const actualRecordingTime = audioRecordingTimeAtSpeechStartRef.current + segment.startTime;
                
                log.debug('ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ (ì¬ê°œ í›„):', {
                  text: segment.text.substring(0, 30) + '...',
                  segmentStartTime: segment.startTime.toFixed(2) + 's',
                  audioTimeAtSpeechStart: audioRecordingTimeAtSpeechStartRef.current.toFixed(2) + 's',
                  actualRecordingTime: actualRecordingTime.toFixed(2) + 's',
                  isPartial: segment.isPartial,
                });

                const words = estimateWordTimestamps(
                  segment.text,
                  actualRecordingTime,
                  segment.confidence || 1.0
                );

                const scriptSegment = {
                  id: segment.isPartial ? `interim-${actualRecordingTime}` : segment.id,
                  timestamp: actualRecordingTime * 1000,
                  originalText: segment.text,
                  words: words,
                  speaker: undefined,
                  isPartial: segment.isPartial,
                };

                // Real-time update: Store all segments (interim + final)
                if (segment.isPartial) {
                  segmentsMapRef.current.set(scriptSegment.id, scriptSegment);
                } else {
                  Array.from(segmentsMapRef.current.keys()).forEach(key => {
                    if (key.startsWith('interim-')) {
                      const interimTime = parseFloat(key.replace('interim-', ''));
                      if (Math.abs(interimTime - actualRecordingTime) < 1.0) {
                        segmentsMapRef.current.delete(key);
                      }
                    }
                  });
                  segmentsMapRef.current.set(scriptSegment.id, scriptSegment);
                  log.debug('ì„¸ê·¸ë¨¼íŠ¸ í™•ì •ë¨:', words.length, 'ê°œ ë‹¨ì–´');
                }

                const allSegments = Array.from(segmentsMapRef.current.values())
                  .sort((a, b) => a.timestamp - b.timestamp);
                setScriptSegments(allSegments);
              },
              onError: (error) => {
                log.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', error);
              },
            }
          );

          speechRecognitionRef.current = speechRecognition;
          speechRecognition.start();
          
          // ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì‹œê°„ê³¼ í˜„ì¬ ì˜¤ë””ì˜¤ ë…¹ìŒ ì‹œê°„ ê¸°ë¡
          speechRecognitionStartTimeRef.current = Date.now();
          audioRecordingTimeAtSpeechStartRef.current = recordingTime; // í˜„ì¬ ë…¹ìŒ ì‹œê°„
          log.debug('ìŒì„± ì¸ì‹ ì¬ì‹œì‘ë¨, ë…¹ìŒ ì‹œê°„:', recordingTime + 's');
        } catch (error) {
          log.error('ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì‹¤íŒ¨:', error);
        }
      }

      // íƒ€ì´ë¨¸ ì¬ê°œ
      timerRef.current = setInterval(() => {
        setRecordingTime((prev) => prev + 1);
      }, 1000);
    }
  }, [recordingTime, setScriptSegments]);

  // ë…¹ìŒ ì¢…ë£Œ ë° ì €ì¥
  const stopRecording = useCallback(async (title?: string): Promise<RecordingData> => {
    return new Promise((resolve, reject) => {
      if (!mediaRecorderRef.current) {
        reject(new Error("ë…¹ìŒì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"));
        return;
      }

      mediaRecorderRef.current.onstop = async () => {
        try {
          // Blob ìƒì„± - ëª¨ë“  chunkë¥¼ ìˆœì„œëŒ€ë¡œ ë³‘í•©
          log.debug('ìµœì¢… ì˜¤ë””ì˜¤ Blob ìƒì„± ì¤‘:', audioChunksRef.current.length, 'ê°œ ì²­í¬');
          const audioBlob = new Blob(audioChunksRef.current, {
            type: mediaRecorderRef.current?.mimeType || "audio/webm",
          });
          log.debug('ìµœì¢… ì˜¤ë””ì˜¤ Blob í¬ê¸°:', audioBlob.size, 'bytes',
                      '| íƒ€ì…:', audioBlob.type);

          // ìŒì„± ì¸ì‹ ì¤‘ì§€
          if (speechRecognitionRef.current) {
            speechRecognitionRef.current.stop();
            speechRecognitionRef.current = null;
            log.debug('ìŒì„± ì¸ì‹ ì¤‘ì§€ë¨');
          }

          // ì„ì‹œ ì„¸ê·¸ë¨¼íŠ¸ ì œê±°, ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ë§Œ ìœ ì§€
          const finalSegments = Array.from(segmentsMapRef.current.values())
            .filter(seg => !seg.isPartial)
            .sort((a, b) => a.timestamp - b.timestamp);

          setScriptSegments(finalSegments);
          log.debug('ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸', finalSegments.length, 'ê°œ ìœ ì§€, ì„ì‹œ ì„¸ê·¸ë¨¼íŠ¸ ì œê±°ë¨');

          // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
          if (streamRef.current) {
            streamRef.current.getTracks().forEach((track) => track.stop());
            streamRef.current = null;
          }

          // íƒ€ì´ë¨¸ ì •ë¦¬
          if (timerRef.current) {
            clearInterval(timerRef.current);
            timerRef.current = null;
          }

          // Generate default title if not provided (using recording start time)
          let recordingTitle = title;
          if (!recordingTitle && recordingStartTime) {
            // Format: YYYY_MM_DD_HH:MM:SS (with leading zeros)
            const year = recordingStartTime.getFullYear();
            const month = String(recordingStartTime.getMonth() + 1).padStart(2, '0');
            const day = String(recordingStartTime.getDate()).padStart(2, '0');
            const hours = String(recordingStartTime.getHours()).padStart(2, '0');
            const minutes = String(recordingStartTime.getMinutes()).padStart(2, '0');
            const seconds = String(recordingStartTime.getSeconds()).padStart(2, '0');
            
            recordingTitle = `${year}_${month}_${day}_${hours}:${minutes}:${seconds}`;
            log.debug('ì‹œì‘ ì‹œê°„ìœ¼ë¡œ ê¸°ë³¸ ì œëª© ìƒì„±:', recordingTitle);
          } else if (!recordingTitle) {
            // Fallback if recordingStartTime is not available
            recordingTitle = `Recording ${new Date().toLocaleString('ko-KR', {
              month: '2-digit',
              day: '2-digit',
              hour: '2-digit',
              minute: '2-digit'
            })}`;
          }

          // ë°±ì—”ë“œì— ì €ì¥ (ì„¸ì…˜ ìƒì„±, ì˜¤ë””ì˜¤ ì—…ë¡œë“œ, ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥)
          setIsSaving(true);
          let sessionId: string | undefined;

          try {
            log.debug('íŠ¸ëœìŠ¤í¬ë¦½ì…˜ ì„¸ì…˜ ìƒì„± ì¤‘:', recordingTitle, 'noteId:', noteId, 'audioRecordingId:', audioRecordingId);

            // 1. ì„¸ì…˜ ìƒì„± (audioRecordingId í¬í•¨í•˜ì—¬ íƒ€ì„ë¼ì¸ ì—°ë™)
            const session = await transcriptionApi.createSession(recordingTitle, noteId || undefined, audioRecordingId || undefined);
            sessionId = session.id;
            log.debug('ì„¸ì…˜ ìƒì„±ë¨:', sessionId, 'audioRecordingId:', audioRecordingId);

            // 2. Convert audio blob to base64 data URL
            const audioDataUrl = await new Promise<string>((resolve) => {
              const reader = new FileReader();
              reader.onloadend = () => resolve(reader.result as string);
              reader.readAsDataURL(audioBlob);
            });

            // 3. ì „ì²´ ì˜¤ë””ì˜¤ ì—…ë¡œë“œ
            log.debug('ì „ì²´ ì˜¤ë””ì˜¤ ì—…ë¡œë“œ ì¤‘...', {
              sessionId,
              audioBlobSize: audioBlob.size,
              audioBlobType: audioBlob.type,
              base64Length: audioDataUrl.length,
              duration: recordingTime,
            });
            const uploadResult = await transcriptionApi.saveFullAudio({
              sessionId,
              audioUrl: audioDataUrl,
              duration: recordingTime,
            });
            log.debug('ì „ì²´ ì˜¤ë””ì˜¤ ì—…ë¡œë“œ ê²°ê³¼:', {
              fullAudioUrl: uploadResult.fullAudioUrl,
              fullAudioKey: uploadResult.fullAudioKey,
              status: uploadResult.status,
            });

            // 4. íŠ¸ëœìŠ¤í¬ë¦½ì…˜ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ê°œì„ )
            if (finalSegments.length > 0 && sessionId) {
              log.debug('íŠ¸ëœìŠ¤í¬ë¦½ì…˜ ì„¸ê·¸ë¨¼íŠ¸', finalSegments.length, 'ê°œ ë³‘ë ¬ ì €ì¥ ì¤‘...');
              const startTime = Date.now();

              // Promise.allë¡œ ë³‘ë ¬ ì²˜ë¦¬ - í›¨ì”¬ ë¹ ë¦„!
              const savePromises = finalSegments.map(async (segment) => {
                try {
                  await transcriptionApi.saveTranscript({
                    sessionId: sessionId!,
                    text: segment.originalText || '',
                    startTime: segment.timestamp / 1000, // Convert ms to seconds
                    endTime: (segment.timestamp / 1000) + ((segment.originalText || '').split(/\s+/).length / 2.5),
                    confidence: 1.0,
                    isPartial: false,
                    language: 'ko',
                    words: segment.words?.map((w: WordWithTime) => ({
                      word: w.word,
                      startTime: w.startTime,
                      confidence: w.confidence || 1.0,
                      wordIndex: w.wordIndex,
                    })),
                  });
                } catch (segmentError) {
                  log.error('ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ ì‹¤íŒ¨:', segmentError);
                  // ë‹¤ë¥¸ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ê³„ì† ì €ì¥
                }
              });

              // ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ ëŒ€ê¸°
              await Promise.all(savePromises);

              const elapsed = Date.now() - startTime;
              log.debug('ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ ì™„ë£Œ:', elapsed + 'ms',
                          '(í‰ê· :', (elapsed / finalSegments.length).toFixed(0) + 'ms/ì„¸ê·¸ë¨¼íŠ¸)');
            }

            // 5. ì„¸ì…˜ ì¢…ë£Œ
            await transcriptionApi.endSession(sessionId);
            log.debug('ì„¸ì…˜ ì¢…ë£Œ ì™„ë£Œ');

            // invalidateQueriesëŠ” use-recording-control.tsì—ì„œ ì²˜ë¦¬í•¨ (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)

          } catch (saveError) {
            log.error('ë°±ì—”ë“œì— ë…¹ìŒ ì €ì¥ ì‹¤íŒ¨:', saveError);
            setError('ë…¹ìŒ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤');
            // ë°±ì—”ë“œ ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¡œì»¬ ë°ì´í„°ë¡œ ê³„ì†
          } finally {
            setIsSaving(false);
          }

          const recordingData: RecordingData = {
            id: Date.now().toString() + Math.random().toString(36).substr(2, 9),
            title: recordingTitle,
            audioBlob,
            duration: recordingTime,
            createdAt: new Date(),
            sessionId, // Include backend session ID
            audioRecordingId: audioRecordingId || undefined, // Include audio recording ID for timeline
          };

          // ìƒíƒœ ì´ˆê¸°í™”
          setIsRecording(false);
          isRecordingRef.current = false; // ğŸ”¥ FIX: Update ref
          setIsPaused(false);
          setRecordingTime(0);
          audioChunksRef.current = [];

          resolve(recordingData);
        } catch (error) {
          log.error('stopRecording ì˜¤ë¥˜:', error);
          reject(error);
        }
      };

      // ë…¹ìŒ ì¢…ë£Œ - /transcription íŒ¨í„´ ë”°ë¼ stop()ë§Œ í˜¸ì¶œ
      // ondataavailableì—ì„œ ìë™ìœ¼ë¡œ ë§ˆì§€ë§‰ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•¨
      log.debug('ë…¹ìŒ ì¤‘ì§€ ì¤‘... í˜„ì¬ ì²­í¬:', audioChunksRef.current.length);
      mediaRecorderRef.current.stop();
    });
  }, [recordingTime, setScriptSegments, queryClient, noteId]);

  // ë…¹ìŒ ì·¨ì†Œ
  const cancelRecording = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
      // ë°ì´í„° ìš”ì²­ ì—†ì´ ì¤‘ì§€ - ì–´ì°¨í”¼ íê¸°í•  ë°ì´í„°
      mediaRecorderRef.current.stop();
      log.debug('ë…¹ìŒ ì·¨ì†Œë¨');
    }

    // ìŒì„± ì¸ì‹ ì¤‘ë‹¨
    if (speechRecognitionRef.current) {
      speechRecognitionRef.current.abort();
      speechRecognitionRef.current = null;
      log.debug('ìŒì„± ì¸ì‹ ì¤‘ë‹¨ë¨');
    }

    // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    // íƒ€ì´ë¨¸ ì •ë¦¬
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }

    // ìŠ¤í¬ë¦½íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ì •ë¦¬
    clearScriptSegments();

    // ìƒíƒœ ì´ˆê¸°í™”
    setIsRecording(false);
    isRecordingRef.current = false; // ref ì—…ë°ì´íŠ¸
    setIsPaused(false);
    setRecordingTime(0);
    audioChunksRef.current = [];
  }, [clearScriptSegments]);

  // ì‹œê°„ í¬ë§·íŒ…
  const formatTime = useCallback((seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}`;
  }, []);

  // í˜ì´ì§€ ì´ë™ ì‹œ ê²½ê³  + ìë™ ì €ì¥
  useEffect(() => {
    if (!isRecording) return;

    log.debug('ğŸ¤ ë…¹ìŒ í™œì„± - beforeunload í•¸ë“¤ëŸ¬ ì„¤ì •');

    let isSavingOnExit = false; // ì¤‘ë³µ ì €ì¥ ë°©ì§€ í”Œë˜ê·¸

    // 1. beforeunload ì´ë²¤íŠ¸ë¡œ ê²½ê³  í‘œì‹œ
    const handleBeforeUnload = (e: BeforeUnloadEvent) => {
      e.preventDefault();
      e.returnValue = ''; // Chrome requires returnValue to be set

      // í˜ì´ì§€ë¥¼ ë– ë‚˜ê¸° ì „ ìë™ ì €ì¥ ì‹œë„ (visibilitychangeê°€ ë¨¼ì € ì²˜ë¦¬ë  ìˆ˜ë„ ìˆìŒ)
      if (!isSavingOnExit && mediaRecorderRef.current?.state !== 'inactive') {
        isSavingOnExit = true;
        log.debug('âš ï¸ beforeunload: ë…¹ìŒ ì €ì¥ ì‹œë„ ì¤‘...');

        // ìë™ ì €ì¥ ì œëª© ìƒì„±
        const autoSaveTitle = recordingStartTime
          ? `ìë™ì €ì¥_${recordingStartTime.getFullYear()}_${String(recordingStartTime.getMonth() + 1).padStart(2, '0')}_${String(recordingStartTime.getDate()).padStart(2, '0')}_${String(recordingStartTime.getHours()).padStart(2, '0')}:${String(recordingStartTime.getMinutes()).padStart(2, '0')}:${String(recordingStartTime.getSeconds()).padStart(2, '0')}`
          : `ìë™ì €ì¥_${new Date().toISOString()}`;

        // ë…¹ìŒ ì¤‘ë‹¨ ë° ì €ì¥ (ë¹„ë™ê¸°ì§€ë§Œ ìµœì„ ì˜ ë…¸ë ¥)
        stopRecording(autoSaveTitle).catch((err) => {
          log.error('beforeunloadì—ì„œ ìë™ ì €ì¥ ì‹¤íŒ¨:', err);
        });
      }

      return ''; // For other browsers
    };

    // 2. visibilitychange ì´ë²¤íŠ¸ë¡œ í˜ì´ì§€ ìˆ¨ê¹€ ì‹œ ìë™ ì €ì¥
    const handleVisibilityChange = async () => {
      // í˜ì´ì§€ê°€ ìˆ¨ê²¨ì§€ê³ (hidden) ë…¹ìŒ ì¤‘ì´ë©´ ìë™ ì €ì¥
      if (document.hidden && isRecording && !isSavingOnExit && mediaRecorderRef.current?.state !== 'inactive') {
        isSavingOnExit = true;
        log.debug('ğŸ‘ï¸ í˜ì´ì§€ ìˆ¨ê¹€ - ë…¹ìŒ ìë™ ì €ì¥ ì¤‘...');

        try {
          // ìë™ ì €ì¥ ì œëª© ìƒì„±
          const autoSaveTitle = recordingStartTime
            ? `ìë™ì €ì¥_${recordingStartTime.getFullYear()}_${String(recordingStartTime.getMonth() + 1).padStart(2, '0')}_${String(recordingStartTime.getDate()).padStart(2, '0')}_${String(recordingStartTime.getHours()).padStart(2, '0')}:${String(recordingStartTime.getMinutes()).padStart(2, '0')}:${String(recordingStartTime.getSeconds()).padStart(2, '0')}`
            : `ìë™ì €ì¥_${new Date().toISOString()}`;

          await stopRecording(autoSaveTitle);
          log.debug('âœ… ë…¹ìŒ ìë™ ì €ì¥ ì„±ê³µ');
        } catch (error) {
          log.error('âŒ visibilitychangeì—ì„œ ìë™ ì €ì¥ ì‹¤íŒ¨:', error);
        }
      }
    };

    window.addEventListener('beforeunload', handleBeforeUnload);
    document.addEventListener('visibilitychange', handleVisibilityChange);

    // 3. ì •ë¦¬: ì»´í¬ë„ŒíŠ¸ unmount ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    return () => {
      log.debug('ğŸ§¹ ì»´í¬ë„ŒíŠ¸ unmount ì¤‘...');
      window.removeEventListener('beforeunload', handleBeforeUnload);
      document.removeEventListener('visibilitychange', handleVisibilityChange);

      // ë…¹ìŒ ì¤‘ì´ë©´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ì €ì¥ì€ beforeunload/visibilitychangeì—ì„œ ì²˜ë¦¬ë¨)
      // stale closure ë°©ì§€ë¥¼ ìœ„í•´ state ëŒ€ì‹  ref ì‚¬ìš©
      if (isRecordingRef.current && !isSavingOnExit) {
        log.debug('âš ï¸ unmount ì¤‘ ë…¹ìŒ í™œì„± - ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘');

        // ìŒì„± ì¸ì‹ ì¤‘ë‹¨
        if (speechRecognitionRef.current) {
          speechRecognitionRef.current.abort();
          speechRecognitionRef.current = null;
          log.debug('ğŸ—£ï¸ ìŒì„± ì¸ì‹ ì¤‘ì§€ë¨');
        }

        // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬ (ë§ˆì´í¬ ë„ê¸°)
        if (streamRef.current) {
          streamRef.current.getTracks().forEach((track) => {
            log.debug('ğŸ¤ ë§ˆì´í¬ íŠ¸ë™ ì¤‘ì§€:', track.label);
            track.stop();
          });
          streamRef.current = null;
        }

        // íƒ€ì´ë¨¸ ì •ë¦¬
        if (timerRef.current) {
          clearInterval(timerRef.current);
          timerRef.current = null;
          log.debug('â±ï¸ íƒ€ì´ë¨¸ ì •ë¦¬ë¨');
        }

        log.debug('âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ');
      }
    };
  }, []); // dependency ì œê±° - mount/unmount ì‹œì—ë§Œ cleanup ì‹¤í–‰

  return {
    isRecording,
    isPaused,
    recordingTime,
    recordingStartTime, // Export recording start time
    formattedTime: formatTime(recordingTime),
    error,
    isSaving,
    audioRecordingId, // Export audio recording ID for timeline events
    startRecording,
    pauseRecording,
    resumeRecording,
    stopRecording,
    cancelRecording,
  };
}