import os
import logging
from typing import Dict
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

from app.utils.database import DatabaseService, FileService
from app.utils.pdf_service import PDFService

logger = logging.getLogger(__name__)


class RAGService:
    """RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        # OpenAI API í‚¤ í™•ì¸
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key or self.api_key == "your_openai_api_key_here":
            logger.warning("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        else:
            logger.info("âœ… OpenAI API í‚¤ í™•ì¸ë¨")
        
        # LlamaIndex ì„¤ì •
        Settings.llm = OpenAI(
            model="gpt-4o-mini",
            api_key=self.api_key,
            temperature=0.7
        )
        Settings.embed_model = OpenAIEmbedding(
            model="text-embedding-3-small",
            api_key=self.api_key
        )
        
        # ì¸ë±ìŠ¤ ìºì‹œ (ë©”ëª¨ë¦¬)
        self.index_cache: Dict[str, VectorStoreIndex] = {}
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.db = DatabaseService()
        self.file_service = FileService()
        self.pdf_service = PDFService()
        
        logger.info("RAGService initialized")
    
    async def get_or_create_index(self, note_id: str) -> VectorStoreIndex:
        """
        ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        PDF íŒŒì¼ì´ ìˆìœ¼ë©´ PDF ê¸°ë°˜, ì—†ìœ¼ë©´ ì „ì‚¬ ë°ì´í„° ê¸°ë°˜
        
        Args:
            note_id: ë…¸íŠ¸ ID
            
        Returns:
            VectorStoreIndex ì¸ìŠ¤í„´ìŠ¤
        """
        # ìºì‹œ í™•ì¸
        if note_id in self.index_cache:
            logger.info(f"âœ… Cache hit for note_id: {note_id}")
            return self.index_cache[note_id]
        
        logger.info(f"ğŸ“¦ Creating new index for note_id: {note_id}")
        
        # 1. ë…¸íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        note_info = await self.db.get_note_info(note_id)
        if not note_info:
            raise ValueError(f"ë…¸íŠ¸ ID '{note_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # 2. PDF íŒŒì¼ì´ ìˆìœ¼ë©´ PDF ê¸°ë°˜, ì—†ìœ¼ë©´ ì „ì‚¬ ë°ì´í„° ê¸°ë°˜
        source_file_url = note_info.get("sourceFileUrl")
        
        # PDF íŒŒì¼ URL ì²´í¬: .pdfë¡œ ëë‚˜ê±°ë‚˜ /download ì—”ë“œí¬ì¸íŠ¸ì¸ ê²½ìš°
        is_pdf = False
        if source_file_url:
            is_pdf = (source_file_url.endswith('.pdf') or 
                     '/download' in source_file_url or
                     'pdf' in source_file_url.lower())
        
        if is_pdf:
            logger.info(f"[INDEX] PDF íŒŒì¼ ë°œê²¬: {source_file_url}")
            index = await self._create_index_from_pdf(note_id, source_file_url)
        else:
            logger.info(f"[INDEX] PDF ì—†ìŒ. ì „ì‚¬ ë°ì´í„° ì‚¬ìš©")
            index = await self._create_index_from_transcripts(note_id)
        
        # ìºì‹œì— ì €ì¥
        self.index_cache[note_id] = index
        logger.info(f"âœ… Index created and cached for note_id: {note_id}")
        
        return index
    
    async def _create_index_from_pdf(self, note_id: str, pdf_url: str) -> VectorStoreIndex:
        """
        PDF íŒŒì¼ë¡œë¶€í„° ì¸ë±ìŠ¤ ìƒì„±
        """
        try:
            # PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            logger.info(f"[PDF] Extracting text from: {pdf_url}")
            pdf_text = await self.pdf_service.extract_text_from_url(pdf_url)
            
            if not pdf_text or len(pdf_text.strip()) < 50:
                raise ValueError("PDFì—ì„œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info(f"[PDF] Extracted {len(pdf_text)} characters")
            
            # Document ìƒì„± (í˜ì´ì§€ë³„ë¡œ ë¶„í• )
            documents = []
            pdf_pages = pdf_text.split("--- Page ")
            
            for i, page_text in enumerate(pdf_pages):
                if page_text.strip():
                    doc = Document(
                        text=page_text.strip(),
                        metadata={
                            "note_id": note_id,
                            "source": "pdf",
                            "page": i
                        }
                    )
                    documents.append(doc)
            
            logger.info(f"[PDF] Created {len(documents)} documents from PDF")
            
            # ì¸ë±ìŠ¤ ìƒì„±
            index = VectorStoreIndex.from_documents(documents)
            logger.info(f"[PDF] âœ… Index created from PDF")
            
            return index
            
        except Exception as e:
            logger.error(f"[PDF] Error creating index from PDF: {e}")
            raise ValueError(f"PDF ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def _create_index_from_transcripts(self, note_id: str) -> VectorStoreIndex:
        """
        ì „ì‚¬ ë°ì´í„°ë¡œë¶€í„° ì¸ë±ìŠ¤ ìƒì„± (ìŒì„± ë…¹ìŒ ê¸°ë°˜)
        """
        try:
            # DBì—ì„œ ì „ì‚¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            transcripts = await self.db.get_transcripts(note_id)
            
            if not transcripts:
                raise ValueError(
                    f"ë…¸íŠ¸ ID '{note_id}'ì— ëŒ€í•œ ì „ì‚¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. "
                    "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì˜¤ë””ì˜¤ë¥¼ ë…¹ìŒí•´ì£¼ì„¸ìš”."
                )
            
            logger.info(f"[TRANSCRIPT] Loaded {len(transcripts)} segments")
            
            # Document ìƒì„±
            documents = []
            for transcript in transcripts:
                doc = Document(
                    text=transcript["text"],
                    metadata={
                        "note_id": note_id,
                        "source": "transcript",
                        "start_sec": float(transcript["startSec"]),
                        "end_sec": float(transcript["endSec"]),
                    }
                )
                documents.append(doc)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            index = VectorStoreIndex.from_documents(documents)
            logger.info(f"[TRANSCRIPT] âœ… Index created from transcripts")
            
            return index
            
        except Exception as e:
            logger.error(f"[TRANSCRIPT] Error: {e}")
            raise
    
    async def ask(self, note_id: str, question: str, use_pdf: bool = True) -> str:
        """
        ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°
        
        Args:
            note_id: ë…¸íŠ¸ ID
            question: ì§ˆë¬¸ ë‚´ìš©
            use_pdf: PDF ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            
        Returns:
            AI ë‹µë³€
        """
        logger.info(f"[ASK] note_id={note_id}, question={question[:50]}..., use_pdf={use_pdf}")
        
        try:
            # ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸° (PDF ë˜ëŠ” ì „ì‚¬ ìë™ ì„ íƒ)
            index = await self.get_or_create_index(note_id)
            
            # ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
            query_engine = index.as_query_engine()
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ê°•ì˜ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ë˜, ìë£Œì—ì„œ ë‹¤ë£¨ì§€ ì•Šì€ ë‚´ìš©ì´ë¼ë©´ ê·¸ë ‡ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            
            # ì¿¼ë¦¬ ì‹¤í–‰
            response = query_engine.query(prompt)
            
            answer = str(response)
            logger.info(f"[ASK] Answer generated ({len(answer)} chars)")
            
            return answer
            
        except Exception as e:
            logger.error(f"[ASK] Error: {e}")
            raise
    
    async def summarize(self, note_id: str, lines: int = 3, use_pdf: bool = True) -> str:
        """
        ê°•ì˜ ë‚´ìš© ìš”ì•½ - ì „ì²´ ë‚´ìš©ì„ ì¼ê´€ë˜ê²Œ ìš”ì•½
        
        Args:
            note_id: ë…¸íŠ¸ ID
            lines: ìš”ì•½í•  ì¤„ ìˆ˜
            use_pdf: PDF ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            
        Returns:
            ìš”ì•½ ë‚´ìš©
        """
        logger.info(f"[SUMMARY] note_id={note_id}, lines={lines}, use_pdf={use_pdf}")
        
        try:
            # ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            index = await self.get_or_create_index(note_id)
            
            # ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (query engine ëŒ€ì‹  ì§ì ‘ ì ‘ê·¼)
            all_texts = []
            docstore = index.docstore
            for doc_id in docstore.docs.keys():
                doc = docstore.get_document(doc_id)
                if doc and doc.text:
                    all_texts.append(doc.text)
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
            full_text = "\n\n".join(all_texts)
            logger.info(f"[SUMMARY] Full text length: {len(full_text)} characters")
            
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìš”ì•½ í›„ ì¬ìš”ì•½
            from llama_index.llms.openai import OpenAI
            llm = OpenAI(model="gpt-4o-mini", api_key=self.api_key, temperature=0.5)
            
            # GPT-4o-miniì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¥¼ ê³ ë ¤ (ì•½ 120K í† í°, ì•ˆì „í•˜ê²Œ 60K ë¬¸ìë¡œ ì œí•œ)
            if len(full_text) > 60000:
                logger.info(f"[SUMMARY] Text too long, using hierarchical summarization")
                summary = await self._hierarchical_summarize(full_text, lines, llm)
            else:
                # í•œ ë²ˆì— ìš”ì•½ ê°€ëŠ¥
                summary = await self._direct_summarize(full_text, lines, llm)
            
            logger.info(f"[SUMMARY] Summary generated ({len(summary)} chars)")
            return summary
            
        except Exception as e:
            logger.error(f"[SUMMARY] Error: {e}")
            raise
    
    async def _direct_summarize(self, text: str, lines: int, llm) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ìš”ì•½ (ì‘ì€ ë¬¸ì„œìš©)
        """
        prompt = f"""ë‹¤ìŒì€ ê°•ì˜ ìë£Œì˜ ì „ì²´ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ì •í™•íˆ {lines}ì¤„ë¡œ ì¼ê´€ì„± ìˆê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ê°•ì˜ ë‚´ìš©:
{text}

ìš”ì•½ ê·œì¹™:
- ì •í™•íˆ {lines}ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
- ë²ˆí˜¸ë‚˜ ê¸°í˜¸ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ í˜•ì‹ìœ¼ë¡œ ì‘ì„±
- ê° ë¬¸ì¥ì€ ìƒˆë¡œìš´ ì¤„ì— ì‘ì„± (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)
- ì „ì²´ ê°•ì˜ì˜ íë¦„ê³¼ ë§¥ë½ì„ ìœ ì§€í•˜ë©° í•µì‹¬ ë‚´ìš©ì„ í¬í•¨
- ê° ë¬¸ì¥ì€ ì„œë¡œ ì—°ê²°ë˜ì–´ í•˜ë‚˜ì˜ ì¼ê´€ëœ ì´ì•¼ê¸°ê°€ ë˜ë„ë¡ ì‘ì„±
- ì£¼ìš” ê°œë…, ì¤‘ìš”í•œ ì •ì˜, í•µì‹¬ ë…¼ì ì„ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨
- í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±
- ë¶€ê°€ ì„¤ëª… ì—†ì´ ìš”ì•½ ë‚´ìš©ë§Œ ì œê³µ

ìš”ì•½:"""
        
        response = await llm.acomplete(prompt)
        return str(response)
    
    async def _hierarchical_summarize(self, text: str, lines: int, llm) -> str:
        """
        ê³„ì¸µì  ìš”ì•½ (í° ë¬¸ì„œìš©)
        1ë‹¨ê³„: í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ê°ê° ìš”ì•½
        2ë‹¨ê³„: ìš”ì•½ë“¤ì„ ë‹¤ì‹œ ì¢…í•©í•˜ì—¬ ìµœì¢… ìš”ì•½
        """
        logger.info("[SUMMARY] Starting hierarchical summarization")
        
        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í•  (ì•½ 20K ë¬¸ìì”©)
        chunk_size = 20000
        chunks = []
        for i in range(0, len(text), chunk_size):
            chunks.append(text[i:i + chunk_size])
        
        logger.info(f"[SUMMARY] Split into {len(chunks)} chunks")
        
        # 2ë‹¨ê³„: ê° ì²­í¬ë¥¼ ìš”ì•½
        chunk_summaries = []
        for i, chunk in enumerate(chunks):
            prompt = f"""ë‹¤ìŒ ê°•ì˜ ìë£Œì˜ ì¼ë¶€ ë‚´ìš©ì„ í•µì‹¬ë§Œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš” (3-5ë¬¸ì¥):

ë‚´ìš©:
{chunk}

ìš”ì•½:"""
            response = await llm.acomplete(prompt)
            chunk_summaries.append(str(response))
            logger.info(f"[SUMMARY] Summarized chunk {i+1}/{len(chunks)}")
        
        # 3ë‹¨ê³„: ì²­í¬ ìš”ì•½ë“¤ì„ ê²°í•©í•˜ì—¬ ìµœì¢… ìš”ì•½ ìƒì„±
        combined_summaries = "\n\n".join(chunk_summaries)
        
        final_prompt = f"""ë‹¤ìŒì€ ê°•ì˜ ìë£Œë¥¼ ì—¬ëŸ¬ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìš”ì•½í•œ ë‚´ìš©ì…ë‹ˆë‹¤. 
ì´ ìš”ì•½ë“¤ì„ ì¢…í•©í•˜ì—¬ ì •í™•íˆ {lines}ì¤„ë¡œ ì¼ê´€ì„± ìˆëŠ” ì „ì²´ ìš”ì•½ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ë¶€ë¶„ ìš”ì•½ë“¤:
{combined_summaries}

ìµœì¢… ìš”ì•½ ê·œì¹™:
- ì •í™•íˆ {lines}ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
- ë²ˆí˜¸ë‚˜ ê¸°í˜¸ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ í˜•ì‹ìœ¼ë¡œ ì‘ì„±
- ê° ë¬¸ì¥ì€ ìƒˆë¡œìš´ ì¤„ì— ì‘ì„± (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)
- ì „ì²´ ê°•ì˜ì˜ íë¦„ê³¼ ë§¥ë½ì„ ìœ ì§€í•˜ë©° í•µì‹¬ ë‚´ìš©ì„ í†µí•©
- ê° ë¬¸ì¥ì€ ì„œë¡œ ì—°ê²°ë˜ì–´ í•˜ë‚˜ì˜ ì¼ê´€ëœ ì´ì•¼ê¸°ê°€ ë˜ë„ë¡ ì‘ì„±
- ì¤‘ë³µëœ ë‚´ìš©ì€ ì œê±°í•˜ê³  ê°€ì¥ ì¤‘ìš”í•œ í•µì‹¬ë§Œ í¬í•¨
- í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±
- ë¶€ê°€ ì„¤ëª… ì—†ì´ ìš”ì•½ ë‚´ìš©ë§Œ ì œê³µ

ìµœì¢… ìš”ì•½:"""
        
        response = await llm.acomplete(final_prompt)
        return str(response)
    
    async def generate_quiz(self, note_id: str, count: int = 5, use_pdf: bool = True) -> list:
        """
        í€´ì¦ˆ ìƒì„±
        
        Args:
            note_id: ë…¸íŠ¸ ID
            count: í€´ì¦ˆ ë¬¸ì œ ìˆ˜
            use_pdf: PDF ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            
        Returns:
            í€´ì¦ˆ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"[QUIZ] note_id={note_id}, count={count}, use_pdf={use_pdf}")
        
        try:
            # ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            index = await self.get_or_create_index(note_id)
            
            # ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
            query_engine = index.as_query_engine()
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ì´ ê°•ì˜ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì‹ í€´ì¦ˆ {count}ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ):
[
  {{
    "question": "ë¬¸ì œ ë‚´ìš©",
    "options": ["ì„ íƒì§€1", "ì„ íƒì§€2", "ì„ íƒì§€3", "ì„ íƒì§€4"],
    "correct_answer": 0,
    "explanation": "ì •ë‹µ í•´ì„¤"
  }}
]

ê·œì¹™:
- ê°•ì˜ì—ì„œ ë‹¤ë£¬ í•µì‹¬ ê°œë…ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¬¸ì œ ì¶œì œ
- ê° ë¬¸ì œëŠ” ë°˜ë“œì‹œ 4ê°œì˜ ì„ íƒì§€ë¥¼ ê°€ì§
- correct_answerëŠ” 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤ (0, 1, 2, 3 ì¤‘ í•˜ë‚˜)
- explanationì€ ì™œ ê·¸ê²ƒì´ ì •ë‹µì¸ì§€ ì„¤ëª…
- ë‚œì´ë„ëŠ” ì¤‘ê°„ ì •ë„ë¡œ
- ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ
- í•œêµ­ì–´ë¡œ ì‘ì„±"""
            
            # ì¿¼ë¦¬ ì‹¤í–‰
            response = query_engine.query(prompt)
            
            response_text = str(response)
            logger.debug(f"[QUIZ] Raw response: {response_text[:200]}...")
            
            # JSON íŒŒì‹± ì‹œë„
            import json
            import re
            
            # JSON ë°°ì—´ ì¶”ì¶œ
            json_match = re.search(r'\[[\s\S]*\]', response_text)
            if json_match:
                json_str = json_match.group(0)
                quizzes = json.loads(json_str)
                
                # ìœ íš¨ì„± ê²€ì‚¬
                valid_quizzes = []
                for quiz in quizzes:
                    if (isinstance(quiz, dict) and
                        'question' in quiz and
                        'options' in quiz and
                        'correct_answer' in quiz and
                        'explanation' in quiz and
                        len(quiz['options']) == 4):
                        valid_quizzes.append(quiz)
                
                logger.info(f"[QUIZ] Generated {len(valid_quizzes)} valid quizzes")
                return valid_quizzes[:count]
            else:
                logger.warning("[QUIZ] No JSON found in response, using fallback")
                return self._generate_fallback_quiz(count)
                
        except Exception as e:
            logger.error(f"[QUIZ] Error: {e}")
            return self._generate_fallback_quiz(count)
    
    def _generate_fallback_quiz(self, count: int) -> list:
        """Fallback í€´ì¦ˆ ìƒì„±"""
        logger.warning(f"[QUIZ] Generating {count} fallback quizzes")
        
        quizzes = []
        for i in range(count):
            quizzes.append({
                "question": f"AIê°€ í€´ì¦ˆë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ë¬¸ì œ {i+1})",
                "options": [
                    "ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”",
                    "ë‹¤ë¥¸ ë…¸íŠ¸ë¥¼ ì„ íƒí•´ë³´ì„¸ìš”",
                    "í€´ì¦ˆ ê°œìˆ˜ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”",
                    "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"
                ],
                "correct_answer": 0,
                "explanation": "AIê°€ í€´ì¦ˆë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ í˜•ì‹ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            })
        
        return quizzes
